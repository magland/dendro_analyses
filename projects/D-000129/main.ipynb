{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cebra example\n",
    "\n",
    "This notebook is for development / experimenting.\n",
    "It is based on https://stes.io/NeuroDataReHack2023/\n",
    "\n",
    "See submit_cebra.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pynwb\n",
    "import dendro.client as den\n",
    "import remfile\n",
    "import h5py\n",
    "\n",
    "\n",
    "# Load project D-000129\n",
    "project = den.load_project(\"9638e926\")\n",
    "dandiset_id = \"000129\"\n",
    "\n",
    "# Select an NWB file\n",
    "asset_path = \"sub-Indy/sub-Indy_desc-train_behavior+ecephys.nwb\"\n",
    "\n",
    "# Lazy load NWB file\n",
    "file = remfile.File(project.get_file(f\"imported/{dandiset_id}/{asset_path}\"))\n",
    "io = pynwb.NWBHDF5IO(file=h5py.File(file, \"r\"), mode=\"r\")\n",
    "nwbfile = io.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nlb_tools.nwb_interface import NWBDataset\n",
    "\n",
    "class Dataset(NWBDataset):\n",
    "\n",
    "    def __init__(self, nwbfile):\n",
    "\n",
    "        super().__init__(nwbfile, \"*train\", split_heldout=False)\n",
    "        # To make computations faster, we will bin the whole dataset into 20ms bins\n",
    "        self.resample(target_bin = 20)\n",
    "\n",
    "        for signal_type in set(self.data.columns.get_level_values(level = 0)):\n",
    "            print(signal_type, self.data[signal_type].shape)\n",
    "            setattr(self, signal_type, self.data[signal_type].values)\n",
    "\n",
    "        values = [tuple(v) for v in self.target_pos]\n",
    "        unique_values = list(sorted(set([v for v in values if not np.isnan(v).any()])))\n",
    "        self.target_pos_idx = np.array([-1 if np.isnan(v).any() else unique_values.index(v) for v in values], dtype = int)\n",
    "\n",
    "dataset = Dataset(nwbfile)\n",
    "\n",
    "print(\"Loaded dataset:\")\n",
    "display(dataset.data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cebra\n",
    "\n",
    "MAX_ITERATIONS = 500\n",
    "\n",
    "def init_model():\n",
    "    return cebra.CEBRA(\n",
    "        # Our selected model will use 10 time bins (200ms) as its input\n",
    "        model_architecture = \"offset10-model\",\n",
    "\n",
    "        # We will use mini-batches of size 1000 for optimization. You should\n",
    "        # generally pick a number greater than 512, and larger values (if they\n",
    "        # fit into memory) are generally better.\n",
    "        batch_size = 1000,\n",
    "\n",
    "        # This is the number of steps to train. I ran an example with 10_000\n",
    "        # which resulted in a usable embedding, but training longer might further\n",
    "        # improve the results\n",
    "        max_iterations = MAX_ITERATIONS,\n",
    "\n",
    "        # This will be the number of output features. The optimal number depends\n",
    "        # on the complexity of the dataset.\n",
    "        output_dimension = 8,\n",
    "\n",
    "        # If you want to see a progress bar during training, specify this\n",
    "        verbose = True\n",
    "\n",
    "        # There are many more parameters to explore. Head to\n",
    "        # https://cebra.ai/docs/api/sklearn/cebra.html to explore them.\n",
    "    )\n",
    "\n",
    "model = init_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_nan = np.isnan(dataset.spikes).any(axis = 1)\n",
    "model.fit(\n",
    "    dataset.spikes[~is_nan],\n",
    "    dataset.cursor_pos[~is_nan]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = model.transform(dataset.spikes[~is_nan])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cebra.plot_loss(model, label = \"Loss curve\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
